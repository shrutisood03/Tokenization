# Tokenization Case Studies in LLMs

This repository contains **two case studies** illustrating how tokenization affects the behavior and performance of large language models (LLMs). Each case study includes examples, token sequences, Python code, and insights.

---

## Case Study 1 â€” GPT-3 and Byte-Level BPE

**Scenario:** GPT-3 uses byte-level BPE to process diverse internet text including emojis, symbols, and non-Latin characters.

**Example Input:**
```
Hello ðŸ‘‹, welcome to AI!
```

**Tokenization (Byte-Level BPE):**

| Token | Meaning |
|-------|---------|
| H | H |
| e | e |
| l | l |
| l | l |
| o | o |
|   | space |
| ðŸ‘‹ | emoji |
| , | comma |
|   | space |
| w | w |
| e | e |
| l | l |
| c | c |
| o | o |
| m | m |
| e | e |
|   | space |
| t | t |
| o | o |
|   | space |
| A | A |
| I | I |
| ! | exclamation |

**Python Hugging Face Example:**
```python
from transformers import GPT2Tokenizer

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")  # GPT-3 uses similar BPE
text = "Hello ðŸ‘‹, welcome to AI!"
tokens = tokenizer.tokenize(text)
print(tokens)
print(tokenizer.convert_tokens_to_ids(tokens))

```

```
Input Text â†’ Byte-Level BPE Tokenizer â†’ Token IDs â†’ Model Input
"Hello ðŸ‘‹" â†’ ['H','e','l','l','o',' ', 'ðŸ‘‹'] â†’ [72,101,108,...] â†’ Embeddings
```

